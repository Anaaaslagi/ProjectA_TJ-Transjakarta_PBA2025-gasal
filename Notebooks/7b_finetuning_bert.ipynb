{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfaca488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (4.56.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (4.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (1.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wede\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Pastikan library terinstall\n",
    "!pip install transformers datasets torch scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fc6de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a2ef821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Total Data Valid: 150\n",
      "                                                text label_str\n",
      "0  Dalam rangka memperingati Hari Perhubungan Nas...   positif\n",
      "1  Sejumlah layananTransjakartapagi ini masih ber...   positif\n",
      "2  Truk mengalami gangguan di Jalan Gatot Subroto...   negatif\n",
      "3  Wakil Gubernur DKI JakartaRano Karnomenargetka...   positif\n",
      "4  Bus listrik Transjakarta sempat mengalami kece...   negatif\n"
     ]
    }
   ],
   "source": [
    "# --- 1. LOAD DATA ---\n",
    "print(\"Loading Data...\")\n",
    "\n",
    "df_text = pd.read_csv('../Data/hasil_preprocessing_bert.csv', sep=',', on_bad_lines='skip')\n",
    "df_label = pd.read_csv('../Data/hasil_pelabelan_transjakarta.csv', sep=',', on_bad_lines='skip')\n",
    "\n",
    "df_text = df_text[['Isi Berita']].rename(columns={'Isi Berita': 'text'})\n",
    "df_label = df_label[['Sentimen']].rename(columns={'Sentimen': 'label_str'})\n",
    "\n",
    "# Samakan jumlah baris (potong jika tidak sama panjang)\n",
    "min_len = min(len(df_text), len(df_label))\n",
    "df = pd.concat([df_text.iloc[:min_len], df_label.iloc[:min_len]], axis=1)\n",
    "\n",
    "# Bersihkan data kosong\n",
    "df = df.dropna()\n",
    "\n",
    "# Normalisasi Label\n",
    "df['label_str'] = df['label_str'].str.lower().str.strip()\n",
    "\n",
    "# Filter hanya label valid\n",
    "valid_labels = ['positif', 'negatif', 'netral']\n",
    "df = df[df['label_str'].isin(valid_labels)].copy()\n",
    "\n",
    "print(f\"Total Data Valid: {len(df)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "477cdfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribusi Kelas: {0: 52, 1: 14, 2: 84}\n",
      "Class Weights (Neg, Net, Pos): tensor([0.9615, 3.5714, 0.5952])\n"
     ]
    }
   ],
   "source": [
    "# --- 2. PERSIAPAN LABEL & WEIGHTS ---\n",
    "# Mapping ke Angka\n",
    "label_map = {'negatif': 0, 'netral': 1, 'positif': 2}\n",
    "df['label'] = df['label_str'].map(label_map)\n",
    "\n",
    "# Hitung Bobot Kelas (Weighted Loss Strategy)\n",
    "# Agar model tidak bias ke kelas mayoritas\n",
    "class_counts = df['label'].value_counts().sort_index()\n",
    "print(\"\\nDistribusi Kelas:\", class_counts.to_dict())\n",
    "\n",
    "total_samples = len(df)\n",
    "n_classes = 3\n",
    "weights = []\n",
    "for i in range(n_classes):\n",
    "    count = class_counts.get(i, 0)\n",
    "    # Rumus: Total / (Jumlah Kelas * Jumlah Sampel per Kelas)\n",
    "    w = total_samples / (n_classes * count) if count > 0 else 1.0\n",
    "    weights.append(w)\n",
    "\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "print(f\"Class Weights (Neg, Net, Pos): {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e36508a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. SPLIT DATA (80% Train, 10% Val, 10% Test) ---\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df['text'].tolist(),\n",
    "    df['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label'] # Jaga proporsi\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts,\n",
    "    temp_labels,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=temp_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. TOKENISASI (IndoBERT) ---\n",
    "PRETRAINED_MODEL = \"indobenchmark/indobert-base-p1\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "class TransJakartaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TransJakartaDataset(train_texts, train_labels)\n",
    "val_dataset = TransJakartaDataset(val_texts, val_labels)\n",
    "test_dataset = TransJakartaDataset(test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c059b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. TOKENISASI & DATASET ---\n",
    "PRETRAINED_MODEL = \"indobenchmark/indobert-base-p1\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "class TransJakartaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TransJakartaDataset(train_texts, train_labels)\n",
    "val_dataset = TransJakartaDataset(val_texts, val_labels)\n",
    "test_dataset = TransJakartaDataset(test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f926882a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. TRAINING ---\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=3).to(device)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_final',\n",
    "    num_train_epochs=4,              # 4 Epoch\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20866cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai Training dengan Weighted Loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wede\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 04:57, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.133426</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.184000</td>\n",
       "      <td>1.033478</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.671296</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.053500</td>\n",
       "      <td>0.937346</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.901754</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.904300</td>\n",
       "      <td>0.753723</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wede\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\Wede\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\Wede\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluasi Final pada Data Test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wede\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1611748933792114, 'eval_accuracy': 0.6, 'eval_f1': 0.541130604288499, 'eval_precision': 0.5060606060606061, 'eval_recall': 0.6, 'eval_runtime': 2.3938, 'eval_samples_per_second': 6.266, 'eval_steps_per_second': 0.418, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_transjakarta_weighted\\\\tokenizer_config.json',\n",
       " './model_transjakarta_weighted\\\\special_tokens_map.json',\n",
       " './model_transjakarta_weighted\\\\vocab.txt',\n",
       " './model_transjakarta_weighted\\\\added_tokens.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 7. MENJALANKAN ---\n",
    "print(\"Memulai Training dengan Weighted Loss...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nEvaluasi Final pada Data Test:\")\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(results)\n",
    "\n",
    "model.save_pretrained(\"./model_transjakarta_weighted\")\n",
    "tokenizer.save_pretrained(\"./model_transjakarta_weighted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
